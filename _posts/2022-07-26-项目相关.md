---
title: 项目相关
date: 2022-08-20 01:55:00 -0500
categories: [笔记]
tags: [操作系统]
pin: false
author: 01

toc: true
comments: true
typora-root-url: ../../Sterben-01.github.io
math: false
mermaid: true
 
---

# 项目相关

# UDP TCP

UDP不需要监听，自然服务端没有listen，UDP是无连接，自然客户端没有connect，服务端也没有accept

![unknown](/assets/blog_res/2022-07-26-%E9%A1%B9%E7%9B%AE%E7%9B%B8%E5%85%B3.assets/unknown.png)

# 同步模型

例子：你是一个老师，让学生做作业，学生做完作业后收作业。

- 同步阻塞：逐个收作业，先收A，再收B，接着是C、D，如果有一个学生还未做完，则你会**等到他写完，然后才继续收下一个**。
- 同步非阻塞：逐个收作业，先收A，再收B，接着是C、D，如果有一个学生还未做完，则你会**跳过该学生，继续去收下一个，收完了一圈再过来问学生写完没，如果没写完就再问一圈。**。
- select/poll：**学生写完了作业会举手，但是你不知道是谁举手，需要一个个的去询问**。
- epoll：**学生写完了作业会举手，你知道是谁举手，你直接去收作业**。

# 同步阻塞：

服务器执行到`accept`的时候，会阻塞，等待直到建立连接为止。服务器执行到`receive（Read）`部分的时候，也会阻塞，等待客户端发送数据。直到客户端发送完数据了才能继续。在这期间所有其他客户端想要建立链接都是不可以的，因为被阻塞在了客户端的`connect`这里。对于这种情况我们可以为每一个`connect`新建一个线程/进程。但是会消耗大量资源。

**写的部分也一样。会阻塞。**

单线程：某个 socket 阻塞，会影响到其他 socket 处理。

多线程：客户端较多时，会造成资源浪费，全部 socket 中可能每个时刻只有几个就绪。同时，线程的调度、上下文切换乃至它们占用的内存，可能都会成为瓶颈。**多线程解决的方式是在主线程`accept`后，新开一个线程对应一个客户端，读和写都在新的线程执行。**

- 一个socket（文件描述符）是否设置为阻塞模式，只会影响到`connect/accept/send/recv`等四个`socketAPI`函数，不会影响到 `select/poll/epoll_wait`函数，后三个函数的超时或者阻塞时间是由其函数自身参数控制的。

# 同步非阻塞（忙轮询）

我们可以把**文件描述符**设为非阻塞。文件描述符设置为非阻塞可以解决`accept/receive(read)/send(write)`的阻塞。在非阻塞模式，所有的建立链接/读/写如果不能执行，则不会阻塞而是继续（循环）执行。所以我们可以把监听文件描述符设置为非阻塞来解决accept阻塞，也可以把读写文件描述符设置为非阻塞解决read write的阻塞。当监听文件描述符检测到连接后，既可以在主线程处理后续逻辑，也可以单开新线程进行处理。但是这种循环执行有个问题。每次循环都需要调用read write这种系统调用，会频繁切换内核态和用户态。浪费资源。

# IO多路复用

## SELECT

**首先，SELECT会拷贝一份我们需要监听的文件描述符集合到内核空间。然后内核帮助我们进行遍历，判断对应的文件描述符是否有修改动作。如果有就给对应的位置置为1，没有就会置为0哪怕原来是1。最后返回有几个事件就绪**。所以这个文件描述符集合每次会被修改。这就是为什么我们代码有两份文件描述符集合。但是假如我们有4个客户端 100 101 102 103需要检测。所以100-103的位置都是1。我们如果此时只有100 和 101 被修改了，那么这个数组里面将只有100 和 101是1。 102和103将会被置为0，不会被继续监听。所以我们有两个数组。一个是原始的只可以被set和clr进行手动设置和归零的。另一个是给内核的，内核可以修改的。所以我们select的时候给内核可修改版本。set和clr还是修改原始版本。然后在每一次的while循环一开始，更新内核版本为原来的需要监听的那几个。

**由于是内核帮助我们进行遍历，然后修改原文件描述符数组。所以最后我们还是要自己遍历一遍文件描述符数组找到到底哪几个文件描述符就绪了。也就是会重复遍历一次。最后我们还需要将自己原来的监听文件描述符数组拷贝回给让内核修改的那份（入参的那一份）**

### 优点：

- 不需要每个 FD 都进行一次系统调用，解决了我们自己使用同步非阻塞的时候频繁使用系统调用导致的频繁的用户态内核态切换问题

### 缺点：

- 有文件描述符数组的拷贝动作，（从用户空间拷贝到内核空间）消耗很大。而且是直接修改后拷贝回用户空间（整个数组全都拷贝）。所以需要两个数组，每次循环后都需要把我们自己保留的一份拷贝给内核的那一份里面。
- 有最大描述符限制
- 内核帮助我们遍历文件描述符数组的时候是线性遍历。所以效率较低
- `select`调用后，我们还是需要再次遍历一次文件描述符数组，找出具体修改的文件描述符。



**SELECT的文件描述符数组其实是bitmap。**

### 请注意。我们所谓的拷贝至内核指的是，将函数的参数拷贝至内核栈。普通参数传递的时候是直接把参数值入栈（用户态或者内核态）。但是系统调用的时候，由于内核不能相信任何用户空间的指针，所以会先把参数写入至寄存器，然后再把参数从寄存器拷贝至内核栈。所以我们SELECT会先把数组拷贝到内核空间，修改后再拷贝回用户空间。

## POLL

和`select`的两个区别

- 去掉了最大监听描述符数量的1024限制
- 不再需要每次重置监听描述符数组（重新赋值）**因为POLL的监听文件描述符数组实际上每个元素是储存了多种信息的结构体，类似`epoll`**

```c++
struct pollfd{
    int		fd;			//委托检测的文件描述符
    short	events;		//委托检测的事件
    short	revents;	//实际发生的事件
}
```

**内核会修改`revents`而不会修改`events`。这是主要区别**

# EPOLL

使用EPOLL的时候我们会先使用`epoll_create`创建一个epoll实例（eventpoll)。**但是这个实例被创建在了内核区**。创建后会返回一个epoll的文件描述符。我们正是通过这个文件描述符操作这个epoll实例。

- **我们所有要求epoll监听的文件描述符被储存在红黑树内。**

- 有一个就绪列表，用来添加所有已经就绪的文件描述符。这是一个双向链表。这里之后会把里面的东西复制回用户空间的一个用于接收就绪文件描述符的数组（`epoll_wait`的第二个参数）
- 有一个等待队列。如果调用时没有时间就绪，就会阻塞，放入等待队列让出CPU以便后续唤醒。

我们每次使用`epoll_ctl`将一个需要监听的文件描述符添加至`eventpoll`的时候，会被封装成`epitem`后拷贝至内核空间，仅需拷贝一次。因为他会一直存在在里面。

- 每一个`epitem`也有一个等待队列，它会关联至一个回调函数(`ep_callback`)。也就是这个`epitem`对应的文件描述符就绪时，会调用这个回调函数来唤醒进程。
- `epoll_wait`的作用是获取就绪的文件描述符。所以项目里面这个函数是放在while里面的。因为每次获取就绪的文件。

## 接收流程

- 服务端通过网卡接收到客户端消息。
- 网卡通过DMA写入内存。
- 发送中断信号给CPU，表示有数据到达。
- CPU调用中断处理程序处理。通过数据包的IP和端口号找到对应的socket套接字（文件描述符）。
- 将数据放入对应socket（文件描述符）的接收队列。
- 处理后会找到对应`epitem`的等待队列关联的回调函数（`ep_poll_callback`)。
- 回调函数会将`epitem`添加至就绪列表。
- 唤醒在等待队列中的进程。（如果进程处于睡眠状态）
- 进程判断就绪列表是否有就绪事件。
- 如果有就绪事件，**拷贝**至用户空间的一个用于接收就绪文件描述符的数组（`epoll_wait`的第二个参数）。

### epoll仅当添加监听的文件描述符（拷贝至内核空间） 和 有对应的就绪事件时（拷贝回用户空间）会发生拷贝。

## 优点

- **epoll对象一直被维护在内核态，所以仅有添加文件描述符时需要进行拷贝**
- **有就绪列表，所以可直接获知就绪事件（文件描述符）。无需重复遍历**
- **监听文件描述符数组储存在红黑树，搜索/添加/删除速度快**

## 缺点：

- 仅支持linux。跨平台性差。
- 比select复杂，移植性差。
- 在监听连接数和事件较少的情况下，select可能更优。因为比较简单。

## LT（水平）/ET（边缘）触发区别

LT模式的时候，只要`epoll_wait`检测到事件没有被处理完毕（比如没有读完），那么后续每次`epoll_wait`调用都会通知。（只要缓冲区有数据就一直触发）

ET模式的时候，`epoll_wait`检测到事件后，仅通知一次。直到下次再次检测到事件后才继续通知。（就算没读完，也要等到下次新的事件到来后才能继续处理）（直到缓冲区数据有增加（变化）才会触发）

**ET模式下可以通知很多次。监听socket不用设置为oneshot是因为只存在一个主线程去操作这个监听socket**

## ONESHOT

oneshot指的某socket对应的fd事件最多只能被检测一次，不论你设置的是读写还是异常。

因为可能存在这种情况：如果epoll检测到了读事件，数据读完交给一个子线程去处理，

如果该线程处理的很慢，在此期间epoll在该socket上又检测到了读事件，则又给了另一个线程去处理，

则在同一时间会存在两个工作线程操作同一个socket。

EPOLLONESHOT这种方法，可以在epoll上注册这个事件，注册这个事件后，如果在处理完毕当前的socket后不再重新注册相关事件，

那么这个事件就不再响应了或者说触发了。

当处理完毕想要通知epoll可以再次处理的时候就要调用epoll_ctl重新注册(重置)文件描述符上的事件。这样前面的socket就不会出现竞态

**也就是说注册了 EPOLLONESHOT 事件的 socket 一旦被某个线程处理完毕， 该线程就应该立即重置这个 socket 上的 EPOLLONESHOT 事件，以确保这个 socket 下一次可读时，其 EPOLLIN 事件能被触发，进而让其他工作线程有机会继续处理这个 socket。**



https://www.jxhs.me/2021/04/08/linux%E5%86%85%E6%A0%B8Epoll-%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/

## 为什么使用ET一定要设置为非阻塞

因为我们ET是一个事件只通知一次，所以为了效率我们必须一次性读完或者写完。我们会在一个单独的while里面进行循环读或者循环写。但是如果我们是阻塞的文件描述符，假设我们在读。我们循环了四次读完了，然后发现没有东西读了。就会一直卡在这直到有新的东西能读。也就是阻塞读的while循环没有退出条件。我们如果设置为非阻塞的话，发现没有数据了会返回一个ERRNO，这样就是有退出条件了。我们就可以`break`退出循环。写也是一个道理。

**尤其是我们这里模拟了preactor模式，用的是主线程进行消息的读取主线程把消息读完了交给子线程去解析，子线程写好了打包好了数据后交给主线程发送。所以read的时候如果用了阻塞的文件描述符，读完了就一直卡在那，也无法接受新链接。彻底卡死。所以设置为非阻塞读，当没有数据的时候不会卡在那，会返回一个错误代码，我们根据错误代码来退出读取的while循环。**

而且，如果是阻塞的，**不循环读**的话会干扰下一次发送的数据，会有上次读不完的和下一次的数据混在一起的事情发生。注意，阻塞与否是要看有没有while。阻塞的核心原因是read函数在阻塞模式下，**循环读**的话没有退出条件。所以：

ET模式下：

- 阻塞不循环：干扰下一次发送的数据
- 阻塞循环：卡死

所以一定要设置非阻塞，然后循环读取。这样既可以保证一次性读完，不会发生数据混乱，也保证了有退出条件。

LT模式下：

因为LT是每次都触发，所以我们不需要设置循环读。我们只要告诉他每次读取几个字节即可。他会一直每次都读取对应的字节数量直到读取完毕。

- 阻塞：一直读直到读完。没问题
- 非阻塞：一样

所以LT模式没什么特别区别

## 如果把监听文件描述符`listenFD`设置为边缘触发的话应该怎么办

如果这样的话，我们也需要把`listenFD`外面加一个`while`循环来循环`accept`所有的链接。不然的话高并发的时候你只处理一个之后他直到新的链接进来都不会通知。服务器无法处理，这样会丢链接。



## recv和send函数作用

**recv和send的作用只是按照规则拷贝而已。从socket缓冲区拷贝到用户缓冲区而已。不管进程是否调用recv()读取socket，对端发来的数据都会经由内核接收并且缓存到socket的内核接收缓冲区之中。recv()所做的工作，就是把内核缓冲区中的数据拷贝到应用层用户的buffer里面，并返回，仅此而已。send（）也一样。所以send将数据拷贝至socket发送缓冲区后会直接返回，而这个时候数据不一定已经成功发送，因为send只负责拷贝。**



## 服务器模型 Proactor和Reactor模式 （事件处理模式）

### Reactor

核心就是主线程**只负责监听**文件描述上是否有事件发生，有的话就立即将该事件通知工作线程。除此之外，主线程不做任何其他实质性的工作。读写数据，接受新的连接，以及处理客户请求均在工作线程中完成。

![QQ截图20220731171623](/assets/blog_res/2022-07-26-%E9%A1%B9%E7%9B%AE%E7%9B%B8%E5%85%B3.assets/QQ%E6%88%AA%E5%9B%BE20220731171623.png)

### Proactor

俩字：异步

![QQ截图20220731171903](/assets/blog_res/2022-07-26-%E9%A1%B9%E7%9B%AE%E7%9B%B8%E5%85%B3.assets/QQ%E6%88%AA%E5%9B%BE20220731171903.png)

## 模拟Proactor模式

主线程负责监听链接 + 读 + 写。读出来的数据交给子线程处理。子线程处理完毕后交给主线程写出（发送）

![QQ截图20220731172002](/assets/blog_res/2022-07-26-%E9%A1%B9%E7%9B%AE%E7%9B%B8%E5%85%B3.assets/QQ%E6%88%AA%E5%9B%BE20220731172002.png)

## 并发模式 - 半同步半反应堆。就是主线程监听+读写，然后封装成任务对象后插入队列交给子线程处理其余的事情

 请求队列用的是链表。因为只需要顺序访问并且需要经常删除和添加。链表速度比较快。







## 客户端断开连接，服务端epoll会监听到EPOLLRDHUP

## 项目问题：

多线程取处理数据用的是信号量。而且信号量只有一个，没有给生产者消费者两个信号量，而是让他俩使用的一个。

这两个可以替换。信号量+锁和条件变量+锁都可以实现对应功能。

## 信号量

然后这个`sem_init`最后的值意思是初始值是几。也不能完全理解为物品容量。信号量后续操作是单纯地对那个数字进行增减。而这个数字没有顶。`wait`会让这个数字减掉1。如果小于0了就挂起。`post`会让睡眠的进程唤醒，如果没有进程睡眠则把数字加1

这也就是为什么信号量是**先等待，后加锁**

- 假设我们使用的是一个信号量，即消费者和生产者共享信号量。
  - 一个信号量的时候就是，如果数字不为0，就该消费消费该生产生产。如果数字为0了，那么消费者就等着，等生产者生产完了通知后继续消费。
  - 首先，初始化的时候，我们不能让消费者直接消费，所以初始化的值一定是0。（如果生产者消费者区分信号量，则生产者信号量初始值应为队列的最大值，消费者信号量初始值仍旧应为0。
  - 生产者：
    - 加锁
    - 生产
    - 解锁
    - post [+1]
  - 消费者：
    - wait [-1] 一定要先等待。如果上来就锁了，因为wait是阻塞的如果是0就阻塞等待，那生产者也拿不到锁也没办法生产了。
    - 加锁
    - 消费
    - 解锁
- 假设我们使用的是一个信号量，即消费者和生产者**不**共享信号量。（其实和条件变量差不多）
  - 首先，初始化的时候，我们不能让消费者直接消费，所以初始化的值一定是0。但是生产者可以直接生产。所以初始化的值可以为队列最大值，比如8。
  - 生产者：
    - wait[-1] 注意这个时候是减掉的生产者自己的空位。也就是每生产一次，减掉一个。他最多生产8个，生产多了就停止等待让这个数字不为0。（消费者会+1）
    - 加锁
    - 生产
    - 解锁
    - post [+1]注意这个时候是添加的消费者的消费。让消费者的信号量不为1
  - 消费者：
    - wait[-1] 注意这个时候是减掉的消费者自己的空位。也就是记录有多少可以消费的
    - 加锁
    - 消费
    - 解锁
    - post[+1]注意这个是告诉生产者+1，也就是可生产的空位+1.

## 条件变量

条件变量一定要先加锁。也可以用一个条件变量也可以用两个。核心是`pthread_cond_wait`。原理是调用方会被挂起（睡眠并加入等待队列），然后互斥锁解锁，让其余线程抢锁，然后调用`notify`通知。通知后`wait`会重新加锁并唤醒当前进程（之后wait函数返回）。系统保证解锁和睡眠是原子操作。系统也保证加锁和唤醒是原子操作。

为了防止虚假唤醒，必须要使用`while`而不能使用wait。

因为我们在wait函数返回之前，当前进程必须执行 拿到锁—>加锁并唤醒 这两步。加锁和唤醒是原子的，但是并不一定能拿得到这个锁。假如我们消费者1在等待，然后生产者生产完毕，通知消费者。假如这个时候消费者2进来了，直接就拿了锁（因为生产者释放锁到wait函数拿锁这两步不是原子的。存在这种第三方插进来的情况）然后消费了生产的数据。然后释放锁。这时候我们消费者1终于拿到锁了，但是发现数据已经被消费了，这样再去拿数据会有错误。所以必须用while。也就是判空。

