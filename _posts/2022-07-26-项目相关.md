---
title: 项目相关
date: 2022-08-20 01:55:00 -0500
categories: [笔记]
tags: [操作系统]
pin: false
author: 01

toc: true
comments: true
typora-root-url: ../../Sterben-01.github.io
math: false
mermaid: true
 
---

# 项目相关

# UDP TCP

UDP不需要监听，自然服务端没有listen，UDP是无连接，自然客户端没有connect，服务端也没有accept

![unknown](/assets/blog_res/2022-07-26-%E9%A1%B9%E7%9B%AE%E7%9B%B8%E5%85%B3.assets/unknown.png)

# 同步模型

例子：你是一个老师，让学生做作业，学生做完作业后收作业。

- 同步阻塞：逐个收作业，先收A，再收B，接着是C、D，如果有一个学生还未做完，则你会**等到他写完，然后才继续收下一个**。
- 同步非阻塞：逐个收作业，先收A，再收B，接着是C、D，如果有一个学生还未做完，则你会**跳过该学生，继续去收下一个，收完了一圈再过来问学生写完没，如果没写完就再问一圈。**。
- select/poll：**学生写完了作业会举手，但是你不知道是谁举手，需要一个个的去询问**。
- epoll：**学生写完了作业会举手，你知道是谁举手，你直接去收作业**。

# 同步阻塞：

服务器执行到`accept`的时候，会阻塞，等待直到建立连接为止。服务器执行到`receive（Read）`部分的时候，也会阻塞，等待客户端发送数据。直到客户端发送完数据了才能继续。在这期间所有其他客户端想要建立链接都是不可以的，因为被阻塞在了客户端的`connect`这里。对于这种情况我们可以为每一个`connect`新建一个线程/进程。但是会消耗大量资源。

**写的部分也一样。会阻塞。**

单线程：某个 socket 阻塞，会影响到其他 socket 处理。

多线程：客户端较多时，会造成资源浪费，全部 socket 中可能每个时刻只有几个就绪。同时，线程的调度、上下文切换乃至它们占用的内存，可能都会成为瓶颈。**多线程解决的方式是在主线程`accept`后，新开一个线程对应一个客户端，读和写都在新的线程执行。**

- 一个socket（文件描述符）是否设置为阻塞模式，只会影响到`connect/accept/send/recv`等四个`socketAPI`函数，不会影响到 `select/poll/epoll_wait`函数，后三个函数的超时或者阻塞时间是由其函数自身参数控制的。

# 同步非阻塞（忙轮询）

我们可以把**文件描述符**设为非阻塞。文件描述符设置为非阻塞可以解决`accept/receive(read)/send(write)`的阻塞。在非阻塞模式，所有的建立链接/读/写如果不能执行，则不会阻塞而是继续（循环）执行。所以我们可以把监听文件描述符设置为非阻塞来解决accept阻塞，也可以把读写文件描述符设置为非阻塞解决read write的阻塞。当监听文件描述符检测到连接后，既可以在主线程处理后续逻辑，也可以单开新线程进行处理。但是这种循环执行有个问题。每次循环都需要调用系统调用来向内核询问是否有数据就绪，会频繁切换内核态和用户态。浪费资源。

# IO多路复用

## SELECT

**首先，SELECT会拷贝一份我们需要监听的文件描述符集合到内核空间。然后内核帮助我们进行遍历，判断对应的文件描述符是否有修改动作。如果有就给对应的位置置为1，没有就会置为0哪怕原来是1。最后返回有几个事件就绪**。所以这个文件描述符集合每次会被修改。这就是为什么我们代码有两份文件描述符集合。但是假如我们有4个客户端 100 101 102 103需要检测。所以100-103的位置都是1。我们如果此时只有100 和 101 被修改了，那么这个数组里面将只有100 和 101是1。 102和103将会被置为0，不会被继续监听。所以我们有两个数组。一个是原始的只可以被set和clr进行手动设置和归零的。另一个是给内核的，内核可以修改的。所以我们select的时候给内核可修改版本。set和clr还是修改原始版本。然后在每一次的while循环一开始，更新内核版本为原来的需要监听的那几个。

**由于是内核帮助我们进行遍历，然后修改原文件描述符数组。所以最后我们还是要自己遍历一遍文件描述符数组找到到底哪几个文件描述符就绪了。也就是会重复遍历一次。最后我们还需要将自己原来的监听文件描述符数组拷贝回给让内核修改的那份（入参的那一份）**

### 优点：

- 不需要每个 FD 都进行一次系统调用，解决了我们自己使用同步非阻塞的时候频繁使用系统调用导致的频繁的用户态内核态切换问题

### 缺点：

- 有文件描述符数组的拷贝动作，（从用户空间拷贝到内核空间）消耗很大。而且是直接修改后拷贝回用户空间（整个数组全都拷贝）。所以需要两个数组，每次循环后都需要把我们自己保留的一份拷贝给内核的那一份里面。
- 有最大描述符限制
- 内核帮助我们遍历文件描述符数组的时候是线性遍历。所以效率较低
- `select`调用后，我们还是需要再次遍历一次文件描述符数组，找出具体修改的文件描述符。



**SELECT的文件描述符数组其实是bitmap。**

### 请注意。我们所谓的拷贝至内核指的是，将函数的参数拷贝至内核栈。普通参数传递的时候是直接把参数值入栈（用户态或者内核态）。但是系统调用的时候，由于内核不能相信任何用户空间的指针，所以会先把参数写入至寄存器，然后再把参数从寄存器拷贝至内核栈。所以我们SELECT会先把数组拷贝到内核空间，修改后再拷贝回用户空间。

## POLL

和`select`的两个区别

- 去掉了最大监听描述符数量的1024限制
- 不再需要每次重置监听描述符数组（重新赋值）**因为POLL的监听文件描述符数组实际上每个元素是储存了多种信息的结构体，类似`epoll`**

```c++
struct pollfd{
    int		fd;			//委托检测的文件描述符
    short	events;		//委托检测的事件
    short	revents;	//实际发生的事件
}
```

**内核会修改`revents`而不会修改`events`。这是主要区别**

# EPOLL

使用EPOLL的时候我们会先使用`epoll_create`创建一个epoll实例（eventpoll)。**但是这个实例被创建在了内核区**。创建后会返回一个epoll的文件描述符。我们正是通过这个文件描述符操作这个epoll实例。

- **我们所有要求epoll监听的文件描述符被储存在红黑树内。**

- 有一个就绪列表，用来添加所有已经就绪的文件描述符。这是一个双向链表。这里之后会把里面的东西复制回用户空间的一个用于接收就绪文件描述符的数组（`epoll_wait`的第二个参数）
- 有一个等待队列。如果调用时没有时间就绪，就会阻塞，放入等待队列让出CPU以便后续唤醒。

我们每次使用`epoll_ctl`将一个需要监听的文件描述符添加至`eventpoll`的时候，会被封装成`epitem`后拷贝至内核空间，仅需拷贝一次。因为他会一直存在在里面。

- 每一个`epitem`也有一个等待队列，它会关联至一个回调函数(`ep_callback`)。也就是这个`epitem`对应的文件描述符就绪时，会调用这个回调函数来唤醒进程。
- `epoll_wait`的作用是获取就绪的文件描述符。所以项目里面这个函数是放在while里面的。因为每次获取就绪的文件。

## 接收流程

- 服务端通过网卡接收到客户端消息。
- 网卡通过DMA写入内存。
- 发送中断信号给CPU，表示有数据到达。
- CPU调用中断处理程序处理。通过数据包的IP和端口号找到对应的socket套接字（文件描述符）。
- 将数据放入对应socket（文件描述符）的接收队列。
- 处理后会找到对应`epitem`的等待队列关联的回调函数（`ep_poll_callback`)。
- 回调函数会将`epitem`添加至就绪列表。
- 唤醒在等待队列中的进程。（如果进程处于睡眠状态）
- 进程判断就绪列表是否有就绪事件。
- 如果有就绪事件，**拷贝**至用户空间的一个用于接收就绪文件描述符的数组（`epoll_wait`的第二个参数）。

### epoll仅当添加监听的文件描述符（拷贝至内核空间） 和 有对应的就绪事件时（拷贝回用户空间）会发生拷贝。

## 优点

- **epoll对象一直被维护在内核态，所以仅有添加文件描述符时需要进行拷贝**
- **有就绪列表，所以可直接获知就绪事件（文件描述符）。无需重复遍历**
- **监听文件描述符数组储存在红黑树，搜索/添加/删除速度快**

## 缺点：

- 仅支持linux。跨平台性差。
- 比select复杂，移植性差。
- 在监听连接数和事件较少的情况下，select可能更优。因为比较简单。

## LT（水平）/ET（边缘）触发区别

LT模式的时候，只要`epoll_wait`检测到事件没有被处理完毕（比如没有读完），那么后续每次`epoll_wait`调用都会通知。（只要缓冲区有数据就一直触发）

ET模式的时候，`epoll_wait`检测到事件后，仅通知一次。直到下次再次检测到事件后才继续通知。（就算没读完，也要等到下次新的事件到来后才能继续处理）（直到缓冲区数据有增加（变化）才会触发）

**ET模式下可以通知很多次。监听socket不用设置为oneshot是因为只存在一个主线程去操作这个监听socket**

## 为什么使用ET

ET减少了重复触发次数，效率会高一些。

## ONESHOT

oneshot指的某socket对应的fd事件最多只能被检测一次，不论你设置的是读写还是异常。

因为可能存在这种情况：如果epoll检测到了读事件，数据读完交给一个子线程去处理，

如果该线程处理的很慢，在此期间epoll在该socket上又检测到了读事件，则又给了另一个线程去处理，

则在同一时间会存在两个工作线程操作同一个socket。

EPOLLONESHOT这种方法，可以在epoll上注册这个事件，注册这个事件后，如果在处理完毕当前的socket后不再重新注册相关事件，

那么这个事件就不再响应了或者说触发了。

当处理完毕想要通知epoll可以再次处理的时候就要调用epoll_ctl重新注册(重置)文件描述符上的事件。这样前面的socket就不会出现竞态

**也就是说注册了 EPOLLONESHOT 事件的 socket 一旦被某个线程处理完毕， 该线程就应该立即重置这个 socket 上的 EPOLLONESHOT 事件，以确保这个 socket 下一次可读时，其 EPOLLIN 事件能被触发，进而让其他工作线程有机会继续处理这个 socket。**



https://www.jxhs.me/2021/04/08/linux%E5%86%85%E6%A0%B8Epoll-%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/

## 为什么使用ET一定要设置为非阻塞

因为我们ET是一个事件只通知一次，所以为了效率我们必须一次性读完或者写完。我们会在一个单独的while里面进行循环读或者循环写。但是如果我们是阻塞的文件描述符，假设我们在读。我们循环了四次读完了，然后发现没有东西读了。就会一直卡在这直到有新的东西能读。也就是阻塞读的while循环没有退出条件。我们如果设置为非阻塞的话，发现没有数据了会返回一个ERRNO，这样就是有退出条件了。我们就可以`break`退出循环。写也是一个道理。

**尤其是我们这里模拟了preactor模式，用的是主线程进行消息的读取主线程把消息读完了交给子线程去解析，子线程写好了打包好了数据后交给主线程发送。所以read的时候如果用了阻塞的文件描述符，读完了就一直卡在那，也无法接受新链接。彻底卡死。所以设置为非阻塞读，当没有数据的时候不会卡在那，会返回一个错误代码，我们根据错误代码来退出读取的while循环。**

而且，如果是阻塞的，**不循环读**的话会干扰下一次发送的数据，会有上次读不完的和下一次的数据混在一起的事情发生。注意，阻塞与否是要看有没有while。阻塞的核心原因是read函数在阻塞模式下，**循环读**的话没有退出条件。所以：

ET模式下：

- 阻塞不循环：干扰下一次发送的数据
- 阻塞循环：卡死

所以一定要设置非阻塞，然后循环读取。这样既可以保证一次性读完，不会发生数据混乱，也保证了有退出条件。

LT模式下：

因为LT是每次都触发，所以我们不需要设置循环读。我们只要告诉他每次读取几个字节即可。他会一直每次都读取对应的字节数量直到读取完毕。

- 阻塞：一直读直到读完。没问题
- 非阻塞：一样

所以LT模式没什么特别区别

## 如果把监听文件描述符`listenFD`设置为边缘触发(ET)的话应该怎么办

如果这样的话，我们也需要把`listenFD`外面加一个`while`循环来循环`accept`所有的链接，直到返回-1而且`errno == EAGAIN`。不然的话高并发的时候你只处理一个之后他直到新的链接进来都不会通知。服务器无法处理，会堵在这，这样会丢链接。

LT模式的监听文件描述符只要`accept`返回不是错误就可以直接建立链接，然后等待下次通知。（只要存在就通知）



## recv和send函数作用

**recv和send的作用只是按照规则拷贝而已。从socket缓冲区拷贝到用户缓冲区而已。不管进程是否调用recv()读取socket，对端发来的数据都会经由内核接收并且缓存到socket的内核接收缓冲区之中。recv()所做的工作，就是把内核缓冲区中的数据拷贝到应用层用户的buffer里面，并返回，仅此而已。send（）也一样。所以send将数据拷贝至socket发送缓冲区后会直接返回，而这个时候数据不一定已经成功发送，因为send只负责拷贝。**



## 服务器模型 Proactor和Reactor模式 （事件处理模式）

### Reactor

核心就是主线程**只负责监听**文件描述上是否有事件发生，有的话就立即将该事件通知工作线程。除此之外，主线程不做任何其他实质性的工作。读写数据，接受新的连接，以及处理客户请求均在工作线程中完成。

![QQ截图20220731171623](/assets/blog_res/2022-07-26-%E9%A1%B9%E7%9B%AE%E7%9B%B8%E5%85%B3.assets/QQ%E6%88%AA%E5%9B%BE20220731171623.png)

### Proactor

俩字：异步

![QQ截图20220731171903](/assets/blog_res/2022-07-26-%E9%A1%B9%E7%9B%AE%E7%9B%B8%E5%85%B3.assets/QQ%E6%88%AA%E5%9B%BE20220731171903.png)

## 模拟Proactor模式

主线程负责监听链接 + 读 + 写。读出来的数据交给子线程处理。子线程处理完毕后交给主线程写出（发送）

![QQ截图20220731172002](/assets/blog_res/2022-07-26-%E9%A1%B9%E7%9B%AE%E7%9B%B8%E5%85%B3.assets/QQ%E6%88%AA%E5%9B%BE20220731172002.png)

## 并发模式 - 半同步半反应堆。就是主线程监听+读写，然后封装成任务对象后插入队列交给子线程处理其余的事情

 请求队列用的是链表。因为只需要顺序访问并且需要经常删除和添加。链表速度比较快。

### 缺点

主线程和子线程共用一个任务队列。主线程和子线程对队列操作都需要加锁，消耗资源较大。

子线程同一时间只能处理一个客户端连接，如果连接数很大，队列会堆积任务导致响应速度越来越慢。就算使用子线程，切换线程也会小号资源。







## 客户端断开连接，服务端epoll会监听到EPOLLRDHUP





## 项目问题：

- 多线程取处理数据用的是信号量。而且信号量只有一个，没有给生产者消费者两个信号量，而是让他俩使用的一个。

- 这两个可以替换。信号量+锁和条件变量+锁都可以实现对应功能。


## 信号量

然后这个`sem_init`最后的值意思是初始值是几。也不能完全理解为物品容量。信号量后续操作是单纯地对那个数字进行增减。而这个数字没有顶。`wait`会让这个数字减掉1。如果小于0了就挂起。`post`会让睡眠的进程唤醒，如果没有进程睡眠则把数字加1

这也就是为什么信号量是**先等待（-1），后加锁**。因为`wait`本身是阻塞的，如果小于0了就挂起。如果先加锁，发现小于0了直接挂起就没办法解锁了。这也是为什么条件变量要反过来。

- 假设我们使用的是一个信号量，即消费者和生产者共享信号量。
  - 一个信号量的时候就是，如果数字不为0，就该消费消费该生产生产。如果数字为0了，那么消费者就等着，等生产者生产完了通知后继续消费。
  - 首先，初始化的时候，我们不能让消费者直接消费，所以初始化的值一定是0。（如果生产者消费者区分信号量，则生产者信号量初始值应为队列的最大值，消费者信号量初始值仍旧应为0。
  - 生产者：
    - 加锁
    - 生产
    - 解锁
    - post [+1]
  - 消费者：
    - wait [-1] 一定要先等待。如果上来就锁了，因为wait是阻塞的如果是0就阻塞等待，那生产者也拿不到锁也没办法生产了。
    - 加锁
    - 消费
    - 解锁
- 假设我们使用的是一个信号量，即消费者和生产者**不**共享信号量。（其实和条件变量差不多）
  - 首先，初始化的时候，我们不能让消费者直接消费，所以初始化的值一定是0。但是生产者可以直接生产。所以初始化的值可以为队列最大值，比如8。
  - 生产者：
    - wait[-1] 注意这个时候是减掉的生产者自己的空位。也就是每生产一次，减掉一个。他最多生产8个，生产多了就停止等待让这个数字不为0。（消费者会+1）
    - 加锁
    - 生产
    - 解锁
    - post [+1]注意这个时候是添加的消费者的消费。让消费者的信号量不为1
  - 消费者：
    - wait[-1] 注意这个时候是减掉的消费者自己的空位。也就是记录有多少可以消费的
    - 加锁
    - 消费
    - 解锁
    - post[+1]注意这个是告诉生产者+1，也就是可生产的空位+1.

## 条件变量

**条件变量一定要先加锁**。也可以用一个条件变量也可以用两个。核心是`pthread_cond_wait`。原理是调用方会被挂起（睡眠并加入等待队列），然后互斥锁解锁，让其余线程抢锁，然后调用`notify`通知。通知后`wait`会重新加锁并唤醒当前进程（之后wait函数返回）。系统保证解锁和睡眠是原子操作。系统也保证加锁和唤醒是原子操作。

为了防止虚假唤醒，必须要使用`while`而不能使用wait。

因为我们在wait函数返回之前，当前进程必须执行 拿到锁—>加锁并唤醒 这两步。加锁和唤醒是原子的，但是并不一定能拿得到这个锁。假如我们消费者1在等待，然后生产者生产完毕，通知消费者。假如这个时候消费者2进来了，直接就拿了锁（因为生产者释放锁到wait函数拿锁这两步不是原子的。存在这种第三方插进来的情况）然后消费了生产的数据。然后释放锁。这时候我们消费者1终于拿到锁了，但是发现数据已经被消费了，这样再去拿数据会有错误。所以必须用while。也就是判空。



## 信号，管道，定时器

我们使用`SIGALRM` 和`SIGTERM`来侦测定时器信息和服务器停止信息。那么我们如何传递呢？

### 使用管道

我们用`socketpair`创建管道。然后从管道写端写入信号值（`send`函数），管道读端注册至`epoll`，通过`epoll`监测读事件。**设置管道写入端为非阻塞。因为如果设置为阻塞，信号套接字（文件描述符）缓冲区满了的话会阻塞。会增加信号处理的时间。**

### 定时器逻辑。

我们main里的第一次`alert`会在五秒后触发，然后信号捕捉函数捕捉到信号，调用`sig_handler`。`sig_handler`会往信号管道写入一个`SIGALRM`数据。然后我们`epoll`监听到信号文件描述符有事件，放入就绪数组。我们遍历至信号文件描述符的时候判断是`SIGALRM`或者是`SIGTRM`。

- 如果是`SIGTRM`则设置`stop`为`true`后停止服务器。然后我们会`delete pool`删除线程池，调用线程池析构函数。析构函数内设置线程停止标识为`true`然后子线程停止。因为是`detach`所以资源自动回收。
- 如果是`SIGALRM`则设置定时变量标志为`true`。我们不立即执行定时任务因为优先级并不高，先处理其余任务比如文件的读写。当一轮文件描述符遍历处理完毕后再处理定时任务。
  - 定时任务即调用`timer_handler`。**由于`alarm`**调用一次只会触发一次，所以函数内仍要设置新的定时器来不断触发信号
    - 第一次在while外面的alarm做为引火器。让我们执行第一次处理信号，因为我们收到了是SIGALRM所以返回true调用此函数。调用此函数后再次设置五秒倒计时。五秒后又会处理信号，又是SIGALRM所以再次调用此函数。如此循环
  - 执行到`timer_handler`后进入`tick`。此函数是判断链表里的任务是否超时。遍历链表。因为我们的链表是升序链表。如果当前链表头的任务仍未超时，则`break`。如果有超时任务，则删除节点并调用回调函数，这个回调函数执行的是：
    - 客户端文件描述符移出`epoll` 
    - 关闭文件描述符

​				然后一直查找超时任务，直到链表头的任务不超时为止。

- 更改时间
  - 注意，我们在`accept`的时候就设置定时器了。所以我们检测到了一个文件描述符有任务了，则把该文件描述符的超时时间重新设置为当前时间+3倍的timeslot。然后调用`adjust_timer`重新设置。设置逻辑也是按照升序的顺序插入到原链表内。**定时器相关操作由主线程执行，所以不会产生共享资源，无需上锁。**
- 复杂度
  - 从实现上看，主要涉及双向链表的插入，删除操作，其中**添加定时器的事件复杂度是O(n),删除定时器的事件复杂度是O(1)。**
- 需要补充的地方，项目没写
  - 写的时候也要重新设置定时器。但是咱们只有单次写入就没事
  - 异常的时候也要从链表中删除对应文件描述符。懒得弄

### 管道传递的是什么类型？

信号本身是整型数值，管道中传递的是ASClI码表中整型数值对应的字符。

switch的变量一般为字符或整型，当switch的变量为字符时，case中可以是字符，也可以是字符对应的ASClIl码。

### 杂项

- 统一事件源
  - 具体的，信号处理函数使用管道将信号传递给主循环，信号处理函数往管道的写端写入信号值，主循环则从管道的读端读出信号值，使用I/O复用系统调用来监听管道读端的可读事件，这样**信号事件与其他文件描述符都可以通过epoll来监测，从而实现统一处理。**
- 定时事件失效也允许，因为定时任务不是必须立刻处理的。



## 异步日志

其实我这个异步日志就是单纯的开了一个线程不断的从阻塞队列取出数据然后写入磁盘。

- 使用了单例模式。多次使用同一个对象不会重复创建，内存中只有一份，大家共享。防止频繁创建导致的内存消耗。

  - 懒汉：真正使用的时候才创建
    - 懒汉在多线程会出现创建多份的情况。虽然后面创建的会覆盖掉前面的，但还要避免。
      - 解决：加锁+双检测。双检锁也可能失效
        - 只用单检测锁的时候每次调用实例的时候都要加锁，影响性能，双检锁的第一层只有在实例没有创建的时候会调用，创建实例后，不是`NULL`所以不用加锁可以直接返回实例。
    - 或者是使用静态局部变量。
  - 饿汉：加载的时候就创建。会消耗更多资源，因为就算没使用到这个实例，但是只要加载了这个类就会创建。

- 阻塞队列使用了生产者消费者模型，使用了锁+条件变量。条件变量上面提到了。

- 再次强调：条件变量先加锁。因为`wait`函数要解锁。

  - 这里必须只能使用`unique_lock`不能使用`lock_guard`。`lock_guard`是阉割版的`unique_lock`，不支持手动解锁。但是`wait`函数要解锁，所以只可以使用`unique_lock`。

- 伪唤醒相关。

- 写入使用了`fflush ` + `fputs`
  - `fflush()`会强迫将缓冲区内的数据写回参数`stream`指定的文件中，如果参数`stream`为`NULL`, `fflush()`会将所有打开的文件数据更新。

  - 在使用多个输出函数连续进行多次输出到控制台时，有可能下一个数据再上一个数据还没输出完毕，还在输出缓冲区中时，下一个printf就把另一个数据加入输出缓冲区，结果冲掉了原来的数据，出现输出错误。在prinf()后加上fflush(stdout);强制马上输出到控制台，可以避免出现上述错误。

- 可变参数宏。

```c++
  #define LOG_BASE(level, format, ...) \
      do {\
          Log* log = Log::Instance();\
          if (log->IsOpen() && log->GetLevel() <= level) {\
              log->write(level, format, ##__VA_ARGS__); \
              log->flush();\
          }\
      } while(0);
  
  #define LOG_DEBUG(format, ...) do {LOG_BASE(0, format, ##__VA_ARGS__)} while(0);
  #define LOG_INFO(format, ...) do {LOG_BASE(1, format, ##__VA_ARGS__)} while(0);
  #define LOG_WARN(format, ...) do {LOG_BASE(2, format, ##__VA_ARGS__)} while(0);
  #define LOG_ERROR(format, ...) do {LOG_BASE(3, format, ##__VA_ARGS__)} while(0);
  
  /*
  这里的do while意思是确保宏可以正确调用。因为宏有先替换再计算的特性。可能会出错
  ...这个可变参数列表在宏中和##__VA_ARGS__搭配使用。
  举例子：
  #define LOG_DEBUG(format, ...) do {LOG_BASE(0, format, ##__VA_ARGS__)} while(0);
  这个意思是你输入LOG_DEBUG("HELLO %d\n", 100)
  会被替换成LOG_BASE(0, "HELLO %d\n", 100)
  
  输入LOG_DEBUG("HELLO %d %s %d\n", 100, "test", 10000)
  会被替换成LOG_BASE(0, "HELLO %d %s %d\n", 100, "test", 10000)
  
  __VA_ARGS__宏前面加上##的作用在于，当可变参数的个数为0时，这里printf参数列表中的的##会把前面多余的”,”去掉，否则会编译出错，建议使用后面这种，使得程序更加健壮。
  */
```

- 日志分文件
  - 日志写入前会判断**当前day**是否为创建日志的时间，行数是否超过最大行限制。
    - 若为创建日志时间，写入日志，**否则按当前时间创建新log**，更新创建时间和行数。
    - 若行数超过最大行限制，在当前日志的末尾加count/max_lines为后缀创建新log将系统信息格式化后输出，具体为:格式化时间＋格式化内容

### **为什么要异步？和同步的区别是什么？**

同步方式写入日志时会产生较多的系统调用，如果某条日志信息过大，会阻塞日志系统，造成系统瓶颈。异步系统采用生产者--消费者模型，具有较高的并发能力。


## 注意事项

- `accept`后拿到的读写文件描述符对应的是TCP请求三次握手后服务器接受连接了。然后这个时候不一定有数据送达。所以真正的报文送达是epoll检测到的读写文件描述符的事件。所以`accept`只是负责把客户端信息封装后放到`epoll`监听队列内。**注意，我们的定时器为了监测链接，所以是从accept的时候就把接收到的读写文件描述符的定时器设置好了。**
- `CHECK_STATE_CONTENT`这个主状态机的状态在咱们项目没用。这个只给POST请求用的。GET没请求体。没数据。

## 项目难点

- 异步日志的加锁。粒度控制
- 日志的拼接。
- 生成响应体的时候计算偏移量

## 什么时候ET 什么时候LT

个人整理：

ET：连接数量较大的时候。假设5000个链接。我们为了不让第1个人速度很快然后队伍越往后延迟越高，（当然应该用消息队列）。我们可以轮询5000个链接，先每一个读1MB，然后循环。

LT：实时性较高，应该尽力处理完毕每一个链接及时返回。

## 客户端什么时候异常断开

客户端出BUG，段错误之类的，或者是接收到一半/发送到一半没信号了等等。

## 为什么用线程池

- 避免创建和销毁线程的开销。（为每个线程的栈分配内存）
- 削峰。如果没有线程池，那么大量链接涌入的时候服务器会同时开启大量的线程，会占用大量内存空间导致内存空间不足，影响服务器稳定性。而且会进行大量的线程切换，开销极大。如果线程数量是固定的，每个线程从队列中取出任务，这样大量涌入的时候不会影响服务器稳定性。而且线程切换是固定的，不会因为频繁新建和销毁线程导致切换不固定。

## 线程池中线程数量

一般是看是IO密集型还是CPU密集型。如果是IO密集型则大小是2N+1, CPU密集型是N+1。

## 主从状态机（HTTP解析）

主状态机从内部调用从状态机，从状态机驱动主状态机。每解析一部分就将状态改变，来完成状态机的解析跳转，最后得到一个完整的HTTP请求。

### 为什么用主从状态机

为了封装逻辑，使得代码逻辑清晰，编程效率高。

### 主状态机

`CHECK_STATE_REQUESTLINE`，解析请求行

- 主状态机的初始状态，调用`parse_line`函数解析行数据
- 调用`process_read_line`解析**请求行**获得请求方法、目标URL及HTTP版本号
- 解析完成后主状态机的状态变为`CHECK_STATE_HEADER`

`CHECK_STATE_HEADER`，解析请求头

- 调用`process_read_headers`函数解析请求头部信息
- 判断是空行还是请求头，若是空行，进而判断`content-length`是否为0，如果不是0，表明是`POST`请求，则状态转移到`CHECK_STATE_CONTENT`，否则说明是`GET`请求，则报文解析结束。
- 若解析的是请求头部字段，则主要分析`connection`字段，`content-length`字段，和`host`字段。其他字段可以直接跳过，各位也可以根据需求继续分析。

`CHECK_STATE_CONTENT`，解析消息体，**仅用于解析POST请求**

### 主状态机的逻辑

主状态机的函数是`process_read`

- 这里的判断条件是
  - 主状态机转移到`CHECK_STATE_CONTENT`，该条件涉及解析消息体。(本项目没用到)
  - 从状态机转移到`LINE_OK`，该条件涉及解析请求行和请求头部
  - 两者为或关系，当条件为真则继续循环，否则退出

- 首先我们调用过了`parse_line`。这个函数帮我们把`\r\n`换成了\0也就是字符串结束符。这样方便读取。主状态机初始状态是`CHECK_STATE_REQUESTLINE`，解析请求行。通过调用从状态机驱动主状态机。


- 解析完请求行后，主状态机继续分析请求头。具体方法上文写了

### 从状态机

三种状态，标识解析一行的读取状态。

- `LINE_OK`，完整读取一行
- `LINE_BAD`，报文语法有误
- `LINE_OPEN`，读取的行不完整


## webbench原理

父进程fork若干个子进程，每个子进程在用户要求时间或默认的时间内对目标web循环发出实际访问请求，父子进程通过管道进行通信，子进程通过管道写端向父进程传递在若干次请求访问完毕后记录到的总信息，父进程通过管道读端读取子进程发来的相关信息，子进程在时间到后结束，父进程在所有子进程退出后统计并给用户显示最后的测试结果，然后退出。



## mmap 零拷贝

### 传统方式进行数据读取和发送

整个过程发生了**4次用户态和内核态的上下文切换**和**4次拷贝**，具体流程如下：

1. 用户进程通过`read()`方法向操作系统发起调用，此时上下文从用户态转向内核态
2. DMA控制器把数据从硬盘中拷贝到读缓冲区
3. CPU把读缓冲区数据拷贝到应用缓冲区，上下文从内核态转为用户态，`read()`返回
4. 用户进程通过`write()`方法发起调用，上下文从用户态转为内核态
5. CPU将应用缓冲区中数据拷贝到socket缓冲区
6. DMA控制器把数据从socket缓冲区拷贝到网卡，上下文从内核态切换回用户态，`write()`返回

![QQ截图20220804011214](/assets/blog_res/2022-07-26-%E9%A1%B9%E7%9B%AE%E7%9B%B8%E5%85%B3.assets/QQ%E6%88%AA%E5%9B%BE20220804011214.png)



### mmap零拷贝

零拷贝并非真的是完全没有数据拷贝的过程，只不过是减少用户态和内核态的切换次数以及CPU拷贝的次数

**mmap+write简单来说就是使用`mmap`替换了read+write中的read操作，减少了一次CPU的拷贝。**

`mmap`主要实现方式是将读缓冲区的地址和用户缓冲区的地址进行映射，内核缓冲区和应用缓冲区共享，从而减少了从读缓冲区到用户缓冲区的一次CPU拷贝。

**也就是说，使用了mmap之后，我们不再需要读了。我们拿到了mmap函数返回的指针（内存首地址）之后我们可以直接在这里修改或者是获取文件了。（mmap函数的形参有被映射文件的文件描述符）**

在这里我们用mmap直接拿到了这个文件映射区的首地址，我们会直接把这个文件地址放到分散写的结构体内，然后用writev直接写入文件读写描述符（客户端文件描述符）即可。

所以整个过程发生了**4次用户态和内核态的上下文切换**和**3次拷贝**，具体流程如下：

1. 用户进程通过`mmap()`方法向操作系统发起调用，上下文从用户态转向内核态
2. DMA控制器把数据从硬盘中拷贝到读缓冲区
3. **上下文从内核态转为用户态，mmap调用返回**
4. 用户进程通过`write()`方法发起调用，上下文从用户态转为内核态
5. **CPU将读缓冲区中数据拷贝到socket缓冲区**
6. DMA控制器把数据从socket缓冲区拷贝到网卡，上下文从内核态切换回用户态，`write()`返回

`mmap`的方式节省了一次CPU拷贝，同时由于用户进程中的内存是虚拟的，只是映射到内核的读缓冲区，所以可以节省一半的内存空间，比较适合大文件的传输。



![QQ截图20220804011220](/assets/blog_res/2022-07-26-%E9%A1%B9%E7%9B%AE%E7%9B%B8%E5%85%B3.assets/QQ%E6%88%AA%E5%9B%BE20220804011220.png)



### sendfile

sendfile简单说就是把write也拿掉了。类似于完美转发。数据不经过用户空间。

**但这种操作仅适用于不需要用户空间读写的情况。比如我们这就不行，我们要写入额外的响应头。这种方法适合于静态文件服务器那种。放到这个项目来说，就是sendfile直接把我们的网站资源发出去了，但是我们的响应数据没办法加进去。**

![QQ截图20220804012822](/assets/blog_res/2022-07-26-%E9%A1%B9%E7%9B%AE%E7%9B%B8%E5%85%B3.assets/QQ%E6%88%AA%E5%9B%BE20220804012822.png)





## 项目介绍

